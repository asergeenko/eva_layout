import streamlit as st
import pandas as pd
import numpy as np
import os
import uuid
from datetime import datetime
from io import BytesIO
import functools
import zipfile
import logging

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MAX_SHEETS_PER_ORDER = 5  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ –Ω–∞ –æ–¥–∏–Ω –∑–∞–∫–∞–∑ (–Ω–æ–º–µ—Ä –∑–∞–∫–∞–∑–∞ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ O)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('eva_layout_debug.log', mode='w', encoding='utf-8'),
        logging.StreamHandler()  # –¢–∞–∫–∂–µ –≤—ã–≤–æ–¥–∏—Ç—å –≤ –∫–æ–Ω—Å–æ–ª—å
    ]
)
logger = logging.getLogger(__name__)

logger.info("=== –ù–ê–ß–ê–õ–û –°–ï–°–°–ò–ò EVA LAYOUT ===")
logger.info(f"MAX_SHEETS_PER_ORDER = {MAX_SHEETS_PER_ORDER}")

# Clear any cached imports (for Streamlit Cloud)
import sys
if 'layout_optimizer' in sys.modules:
    del sys.modules['layout_optimizer']

# Force cache clear
import importlib
try:
    import layout_optimizer
    importlib.reload(layout_optimizer)
except:
    pass

try:
    from layout_optimizer import (
        parse_dxf_complete, 
        save_dxf_layout_complete,
        parse_dxf, 
        bin_packing, 
        bin_packing_with_inventory, 
        save_dxf_layout, 
        plot_layout, 
        plot_input_polygons, 
        scale_polygons_to_fit
    )
    st.success("‚úÖ –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã")
except ImportError as e:
    # Debug information
    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    
    # Fallback import with debug
    try:
        import layout_optimizer as lo
        st.info(f"–ú–æ–¥—É–ª—å layout_optimizer –∑–∞–≥—Ä—É–∂–µ–Ω. –í–µ—Ä—Å–∏—è: {getattr(lo, '__version__', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')}")
        
        # Show available functions
        available_funcs = [attr for attr in dir(lo) if not attr.startswith('_') and callable(getattr(lo, attr))]
        st.info(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏: {', '.join(available_funcs)}")
        
        # Check specific function
        if hasattr(lo, 'bin_packing_with_inventory'):
            st.success("‚úÖ bin_packing_with_inventory –Ω–∞–π–¥–µ–Ω–∞!")
            bin_packing_with_inventory = lo.bin_packing_with_inventory
        else:
            st.error("‚ùå bin_packing_with_inventory –ù–ï –Ω–∞–π–¥–µ–Ω–∞!")
            
            # Try alternative approach - define the function inline as a workaround
            st.warning("üîß –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ö–æ–¥–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ...")
            
            def bin_packing_with_inventory_fallback(polygons, available_sheets, verbose=True, max_sheets_per_order=None):
                """Fallback implementation if import fails."""
                if verbose:
                    st.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è bin_packing_with_inventory")
                
                placed_layouts = []
                unplaced = polygons.copy()
                sheet_counter = 0
                
                for sheet in available_sheets:
                    if not unplaced:
                        break
                    
                    available_count = sheet['count'] - sheet['used']
                    for _ in range(available_count):
                        if not unplaced:
                            break
                            
                        sheet_counter += 1
                        sheet_size = (sheet['width'], sheet['height'])
                        
                        placed, remaining = lo.bin_packing(unplaced, sheet_size, verbose=verbose)
                        
                        if placed:
                            usage_percent = lo.calculate_usage_percent(placed, sheet_size)
                            placed_layouts.append({
                                'sheet_number': sheet_counter,
                                'sheet_type': sheet['name'],
                                'sheet_size': sheet_size,
                                'placed_polygons': placed,
                                'usage_percent': usage_percent
                            })
                            unplaced = remaining
                        else:
                            break
                
                return placed_layouts, unplaced
                
            bin_packing_with_inventory = bin_packing_with_inventory_fallback
            st.success("‚úÖ –†–µ–∑–µ—Ä–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è bin_packing_with_inventory —Å–æ–∑–¥–∞–Ω–∞!")
            
        # Assign other functions
        parse_dxf_complete = lo.parse_dxf_complete
        save_dxf_layout_complete = lo.save_dxf_layout_complete
        parse_dxf = lo.parse_dxf
        bin_packing = lo.bin_packing
        save_dxf_layout = lo.save_dxf_layout
        plot_layout = lo.plot_layout
        plot_input_polygons = lo.plot_input_polygons
        scale_polygons_to_fit = lo.scale_polygons_to_fit
        
    except Exception as e2:
        st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e2}")
        st.error("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª layout_optimizer.py –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏ –¥–æ—Å—Ç—É–ø–µ–Ω")
        st.stop()

# Configuration
DEFAULT_SHEET_TYPES = [
    (140, 200), (142, 200), (144, 200), (146, 200), (148, 200),
    (140, 195), (142, 195), (144, 195), (146, 195), (148, 195),
    (100, 100), (150, 150), (200, 300)
]
OUTPUT_FOLDER = "output_layouts"
os.makedirs(OUTPUT_FOLDER, exist_ok=True)


# Streamlit App
st.title("–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Ä–∞—Å–∫—Ä–æ—è EVA –∫–æ–≤—Ä–∏–∫–æ–≤")
st.write("–ó–∞–≥—Ä—É–∑–∏—Ç–µ DXF —Ñ–∞–π–ª—ã –∏ —É–∫–∞–∂–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –ª–∏—Å—Ç–∞ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –∫–æ–≤—Ä–∏–∫–æ–≤.")

# Sheet Inventory Section
st.header("üìã –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ª–∏—Å—Ç–æ–≤")
st.write("–£–∫–∞–∂–∏—Ç–µ –∫–∞–∫–∏–µ –ª–∏—Å—Ç—ã —É –≤–∞—Å –µ—Å—Ç—å –≤ –Ω–∞–ª–∏—á–∏–∏ –∏ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ.")

# Initialize session state for sheets
if 'available_sheets' not in st.session_state:
    st.session_state.available_sheets = []

# Update existing sheets to have color if missing (for backward compatibility)
for sheet in st.session_state.available_sheets:
    if 'color' not in sheet:
        sheet['color'] = '—Å–µ—Ä—ã–π'  # Default color for existing sheets

# Add new sheet type
st.subheader("–î–æ–±–∞–≤–∏—Ç—å —Ç–∏–ø –ª–∏—Å—Ç–∞")
col1, col2, col3 = st.columns([2, 2, 1])

with col1:
    sheet_type_option = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –ª–∏—Å—Ç–∞ (—Å–º)", 
        ["–ü—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π"] + [f"{w}x{h}" for w, h in DEFAULT_SHEET_TYPES],
        key="sheet_type_select"
    )
    
    if sheet_type_option == "–ü—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π":
        sheet_width = st.number_input("–®–∏—Ä–∏–Ω–∞ (—Å–º)", min_value=50.0, max_value=300.0, value=140.0, key="custom_width")
        sheet_height = st.number_input("–í—ã—Å–æ—Ç–∞ (—Å–º)", min_value=50.0, max_value=300.0, value=200.0, key="custom_height")
        selected_size = (sheet_width, sheet_height)
    else:
        selected_size = tuple(map(float, sheet_type_option.split("x")))
        
    # Color selection
    sheet_color = st.selectbox(
        "–¶–≤–µ—Ç –ª–∏—Å—Ç–∞", 
        ["—á—ë—Ä–Ω—ã–π", "—Å–µ—Ä—ã–π"],
        key="sheet_color"
    )
        
with col2:
    sheet_count = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤", min_value=1, max_value=100, value=5, key="sheet_count")
    sheet_name = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –ª–∏—Å—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", 
                              value=f"–õ–∏—Å—Ç {selected_size[0]}x{selected_size[1]} {sheet_color}", 
                              key="sheet_name")

with col3:
    st.write("")
    st.write("")
    if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å", key="add_sheet"):
        new_sheet = {
            "name": sheet_name,
            "width": selected_size[0],
            "height": selected_size[1], 
            "color": sheet_color,
            "count": sheet_count,
            "used": 0
        }
        st.session_state.available_sheets.append(new_sheet)
        st.success(f"–î–æ–±–∞–≤–ª–µ–Ω —Ç–∏–ø –ª–∏—Å—Ç–∞: {new_sheet['name']} ({new_sheet['count']} —à—Ç.)")
        st.rerun()

# Display current sheet inventory
if st.session_state.available_sheets:
    st.subheader("üìä –î–æ—Å—Ç—É–ø–Ω—ã–µ –ª–∏—Å—Ç—ã")
    
    # Create DataFrame for display
    sheets_data = []
    total_sheets = 0
    for i, sheet in enumerate(st.session_state.available_sheets):
        # Add color indicator
        color = sheet.get('color', '–Ω–µ —É–∫–∞–∑–∞–Ω')
        color_emoji = "‚ö´" if color == "—á—ë—Ä–Ω—ã–π" else "‚ö™" if color == "—Å–µ—Ä—ã–π" else "üîò"
        color_display = f"{color_emoji} {color}"
        
        sheets_data.append({
            "‚Ññ": i + 1,
            "–ù–∞–∑–≤–∞–Ω–∏–µ": sheet['name'],
            "–†–∞–∑–º–µ—Ä (—Å–º)": f"{sheet['width']}x{sheet['height']}",
            "–¶–≤–µ—Ç": color_display,
            "–î–æ—Å—Ç—É–ø–Ω–æ": f"{sheet['count'] - sheet['used']}/{sheet['count']}",
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ": sheet['used']
        })
        total_sheets += sheet['count']
    
    sheets_df = pd.DataFrame(sheets_data)
    st.dataframe(sheets_df, use_container_width=True)
    
    col1, col2 = st.columns([3, 1])
    with col1:
        st.metric("–í—Å–µ–≥–æ –ª–∏—Å—Ç–æ–≤ –≤ –Ω–∞–ª–∏—á–∏–∏", total_sheets)
    with col2:
        if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ", key="clear_sheets"):
            st.session_state.available_sheets = []
            st.rerun()
else:
    st.info("–î–æ–±–∞–≤—å—Ç–µ —Ç–∏–ø—ã –ª–∏—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —É –≤–∞—Å –µ—Å—Ç—å –≤ –Ω–∞–ª–∏—á–∏–∏.")

# Order Loading Section
st.header("üìã –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–∫–∞–∑–æ–≤ –∏–∑ Excel —Ç–∞–±–ª–∏—Ü—ã")

# Initialize session state for orders
if 'selected_orders' not in st.session_state:
    st.session_state.selected_orders = []
if 'manual_files' not in st.session_state:
    st.session_state.manual_files = []

# Excel file upload
excel_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –∑–∞–∫–∞–∑–æ–≤ Excel", type=["xlsx", "xls"], key="excel_upload")

@st.cache_data(ttl=600)  # Cache for 10 minutes
def load_excel_file(file_content):
    """Load and cache Excel file processing"""
    excel_data = pd.read_excel(BytesIO(file_content), sheet_name=None, header=None, 
                              date_format=None, parse_dates=False)
    return excel_data

if excel_file is not None:
    try:
        with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ Excel —Ñ–∞–π–ª–∞..."):
            # Read Excel file with caching
            file_content = excel_file.read()
            excel_data = load_excel_file(file_content)
            logger.info(f"Excel —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω. –õ–∏—Å—Ç—ã: {list(excel_data.keys())}")
        
        # Get current month name and previous month
        from datetime import datetime
        current_date = datetime.now()
        current_month = current_date.strftime("%B %Y").upper()
    
        # Russian month names mapping
        month_mapping = {
            "JANUARY": "–Ø–ù–í–ê–†–¨", "FEBRUARY": "–§–ï–í–†–ê–õ–¨", "MARCH": "–ú–ê–†–¢", 
            "APRIL": "–ê–ü–†–ï–õ–¨", "MAY": "–ú–ê–ô", "JUNE": "–ò–Æ–ù–¨",
            "JULY": "–ò–Æ–õ–¨", "AUGUST": "–ê–í–ì–£–°–¢", "SEPTEMBER": "–°–ï–ù–¢–Ø–ë–†–¨",
            "OCTOBER": "–û–ö–¢–Ø–ë–†–¨", "NOVEMBER": "–ù–û–Ø–ë–†–¨", "DECEMBER": "–î–ï–ö–ê–ë–†–¨"
        }
        
        current_month_ru = month_mapping.get(current_date.strftime("%B").upper(), "–ò–Æ–õ–¨") + " " + str(current_date.year)
        
        # Get previous month (handle day overflow correctly)
        if current_date.month == 1:
            prev_month_ru = "–î–ï–ö–ê–ë–†–¨ " + str(current_date.year - 1)
        else:
            # Use first day of month to avoid day overflow issues
            first_of_current_month = current_date.replace(day=1)
            prev_date = first_of_current_month.replace(month=first_of_current_month.month - 1)
            prev_month_ru = month_mapping.get(prev_date.strftime("%B").upper(), "–ò–Æ–ù–¨") + " " + str(prev_date.year)
        
        target_sheets = [current_month_ru, prev_month_ru]
        
        st.info(f"üóìÔ∏è –ò—â–µ–º –ª–∏—Å—Ç—ã –¥–ª—è: {', '.join(target_sheets)}")
        
        # Process target sheets
        all_orders = []
        for sheet_name in target_sheets:
            if sheet_name in excel_data:
                df = excel_data[sheet_name]
                
                # Skip first 2 rows (headers), start from row 2 (index 2)
                if df.shape[0] > 2:
                    data_rows = df.iloc[2:].copy()
                    
                    # Check for empty "–°–¥–µ–ª–∞–Ω–æ" column (index 2)
                    if df.shape[1] > 3:  # Make sure we have enough columns
                        pending_orders = data_rows[data_rows.iloc[:, 2].isna() | (data_rows.iloc[:, 2] == '')]
                        
                        for idx, row in pending_orders.iterrows():
                            if pd.notna(row.iloc[3]):  # Check if –ê—Ä—Ç–∏–∫—É–ª (column D) is not empty
                                # Get color from column I (index 8)
                                color = str(row.iloc[8]).lower().strip() if pd.notna(row.iloc[8]) and df.shape[1] > 8 else ''
                                # Normalize color values
                                if '—á–µ—Ä–Ω' in color or 'black' in color:
                                    color = '—á—ë—Ä–Ω—ã–π'
                                elif '—Å–µ—Ä' in color or 'gray' in color or 'grey' in color:
                                    color = '—Å–µ—Ä—ã–π'
                                else:
                                    color = '—Å–µ—Ä—ã–π'  # Default color if not specified
                                
                                order = {
                                    'sheet': sheet_name,
                                    'row_index': idx,
                                    'date': str(row.iloc[0]) if pd.notna(row.iloc[0]) else '',
                                    'article': str(row.iloc[3]),
                                    'product': str(row.iloc[4]) if pd.notna(row.iloc[4]) else '',
                                    'client': str(row.iloc[5]) if pd.notna(row.iloc[5]) else '' if df.shape[1] > 5 else '',
                                    'order_id': str(row.iloc[14]) if pd.notna(row.iloc[14]) and df.shape[1] > 13 else '',
                                    'color': color
                                }
                                all_orders.append(order)
        
        if all_orders:
            st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(all_orders)} –Ω–µ–≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤")
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(all_orders)} –Ω–µ–≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ –≤ Excel")
            
            # Display orders for selection
            st.subheader("üìù –í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–∫–∞–∑—ã –¥–ª—è —Ä–∞—Å–∫—Ä–æ—è")
            
            # Add search/filter options
            col_filter1, col_filter2 = st.columns([1, 1])
            with col_filter1:
                search_article = st.text_input("üîç –ü–æ–∏—Å–∫ –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É:", placeholder="–í–≤–µ–¥–∏—Ç–µ —á–∞—Å—Ç—å –∞—Ä—Ç–∏–∫—É–ª–∞", key="search_article")
            with col_filter2:
                search_product = st.text_input("üîç –ü–æ–∏—Å–∫ –ø–æ —Ç–æ–≤–∞—Ä—É:", placeholder="–í–≤–µ–¥–∏—Ç–µ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è", key="search_product")
            
            # Filter orders based on search
            filtered_orders = all_orders
            if search_article:
                filtered_orders = [order for order in filtered_orders if search_article.lower() in order['article'].lower()]
            if search_product:
                filtered_orders = [order for order in filtered_orders if search_product.lower() in order['product'].lower()]
            
            if filtered_orders != all_orders:
                st.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(filtered_orders)} –∑–∞–∫–∞–∑–æ–≤ –∏–∑ {len(all_orders)} (–ø—Ä–∏–º–µ–Ω–µ–Ω—ã —Ñ–∏–ª—å—Ç—Ä—ã)")
            
            # Update all_orders with filtered results for display
            display_orders = filtered_orders
        
            # Create selection interface
            selected_indices = []
            
            # Show orders in batches of 10 for better UX
            orders_per_page = 10
            total_pages = (len(display_orders) + orders_per_page - 1) // orders_per_page
            
            if total_pages > 1:
                page = st.selectbox("–°—Ç—Ä–∞–Ω–∏—Ü–∞", range(1, total_pages + 1), key="orders_page") - 1
                start_idx = page * orders_per_page
                end_idx = min(start_idx + orders_per_page, len(display_orders))
                orders_to_show = display_orders[start_idx:end_idx]
            else:
                orders_to_show = display_orders
                start_idx = 0
            
            # Create table data for orders
            table_data = []
            for i, order in enumerate(orders_to_show):
                actual_idx = start_idx + i
                # Get checkbox state
                is_selected = st.session_state.get(f"order_{actual_idx}", False)
                
                # Add color emoji for display
                color = order.get('color', '—Å–µ—Ä—ã–π')
                color_emoji = "‚ö´" if color == "—á—ë—Ä–Ω—ã–π" else "‚ö™" if color == "—Å–µ—Ä—ã–π" else "üîò"
                color_display = f"{color_emoji} {color}"
                
                table_data.append({
                    "–í—ã–±—Ä–∞—Ç—å": is_selected,
                    "–ê—Ä—Ç–∏–∫—É–ª": order['article'],
                    "–¢–æ–≤–∞—Ä": order['product'][:50] + "..." if len(order['product']) > 50 else order['product'],
                    "–¶–≤–µ—Ç": color_display,
                    "–ö–ª–∏–µ–Ω—Ç": order.get('client', '')[:25] + "..." if len(order.get('client', '')) > 25 else order.get('client', ''),
                    "–î–∞—Ç–∞": order.get('date', '')[:10],  # Show only date part
                    "–ú–µ—Å—è—Ü": order['sheet'],
                    "‚Ññ –∑–∞–∫–∞–∑–∞": order.get('order_id', '')
                })
            
            # Display orders table with selection
            if table_data:
                orders_df = pd.DataFrame(table_data)
                
                # Use data_editor for selection
                edited_df = st.data_editor(
                    orders_df,
                    column_config={
                        "–í—ã–±—Ä–∞—Ç—å": st.column_config.CheckboxColumn(
                            "–í—ã–±—Ä–∞—Ç—å",
                            help="–û—Ç–º–µ—Ç—å—Ç–µ –∑–∞–∫–∞–∑—ã –¥–ª—è —Ä–∞—Å–∫—Ä–æ—è",
                            default=False,
                        ),
                        "–ê—Ä—Ç–∏–∫—É–ª": st.column_config.TextColumn(
                            "–ê—Ä—Ç–∏–∫—É–ª",
                            help="–ö–æ–¥ –∞—Ä—Ç–∏–∫—É–ª–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ DXF —Ñ–∞–π–ª–æ–≤",
                            width="medium"
                        ),
                        "–¢–æ–≤–∞—Ä": st.column_config.TextColumn(
                            "–¢–æ–≤–∞—Ä",
                            help="–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞",
                            width="large"
                        ),
                        "–¶–≤–µ—Ç": st.column_config.TextColumn(
                            "–¶–≤–µ—Ç",
                            help="–¶–≤–µ—Ç –ª–∏—Å—Ç–∞ –¥–ª—è –∑–∞–∫–∞–∑–∞",
                            width="small"
                        ),
                        "–ö–ª–∏–µ–Ω—Ç": st.column_config.TextColumn(
                            "–ö–ª–∏–µ–Ω—Ç",
                            help="–ù–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞",
                            width="medium"
                        ),
                        "–î–∞—Ç–∞": st.column_config.TextColumn(
                            "–î–∞—Ç–∞",
                            help="–î–∞—Ç–∞ –∑–∞–∫–∞–∑–∞",
                            width="small"
                        ),
                        "–ú–µ—Å—è—Ü": st.column_config.TextColumn(
                            "–ú–µ—Å—è—Ü",
                            help="–ú–µ—Å—è—Ü –∏–∑ Excel –ª–∏—Å—Ç–∞",
                            width="small"
                        ),
                        "‚Ññ –∑–∞–∫–∞–∑–∞": st.column_config.TextColumn(
                            "‚Ññ –∑–∞–∫–∞–∑–∞",
                            help="–ù–æ–º–µ—Ä –∑–∞–∫–∞–∑–∞",
                            width="small"
                        )
                    },
                    disabled=["–ê—Ä—Ç–∏–∫—É–ª", "–¢–æ–≤–∞—Ä", "–¶–≤–µ—Ç", "–ö–ª–∏–µ–Ω—Ç", "–î–∞—Ç–∞", "–ú–µ—Å—è—Ü", "‚Ññ –∑–∞–∫–∞–∑–∞"],
                    hide_index=True,
                    use_container_width=True,
                    key=f"orders_table_page_{start_idx}"
                )
                
                # Update session state based on table selections
                for i, row in edited_df.iterrows():
                    actual_idx = start_idx + i
                    st.session_state[f"order_{actual_idx}"] = row["–í—ã–±—Ä–∞—Ç—å"]
            
            # Bulk selection controls
            col1, col2, col3 = st.columns([1, 1, 2])
            with col1:
                if st.button("‚úÖ –í—ã–±—Ä–∞—Ç—å –≤—Å–µ", key=f"select_all_{start_idx}"):
                    for i in range(len(orders_to_show)):
                        st.session_state[f"order_{start_idx + i}"] = True
                    # Mark that we just performed bulk selection to avoid double processing
                    st.session_state[f"bulk_selected_{start_idx}"] = True
                    st.rerun()
            with col2:
                if st.button("‚ùå –°–Ω—è—Ç—å –≤—Å–µ", key=f"deselect_all_{start_idx}"):
                    for i in range(len(orders_to_show)):
                        st.session_state[f"order_{start_idx + i}"] = False
                    st.rerun()
            
            # Collect all selected orders from all pages
            all_selected_orders = []
            for i in range(len(all_orders)):
                if st.session_state.get(f"order_{i}", False):
                    all_selected_orders.append(all_orders[i])
            
            if all_selected_orders:
                st.success(f"üéØ –í—ã–±—Ä–∞–Ω–æ –∑–∞–∫–∞–∑–æ–≤: {len(all_selected_orders)}")
                
                # Show selected orders summary in table format (only for reasonable number of orders)
                if len(all_selected_orders) <= 20:
                    with st.expander("üìã –í—ã–±—Ä–∞–Ω–Ω—ã–µ –∑–∞–∫–∞–∑—ã", expanded=False):
                        selected_summary = []
                        for order in all_selected_orders:
                            selected_summary.append({
                                "–ê—Ä—Ç–∏–∫—É–ª": order['article'],
                                "–¢–æ–≤–∞—Ä": order['product'][:40] + "..." if len(order['product']) > 40 else order['product'],
                                "–ú–µ—Å—è—Ü": order['sheet']
                                })
                        
                        selected_df = pd.DataFrame(selected_summary)
                        st.dataframe(selected_df, use_container_width=True, hide_index=True)
                else:
                    st.info(f"üìã –í—ã–±—Ä–∞–Ω–æ {len(all_selected_orders)} –∑–∞–∫–∞–∑–æ–≤ (—Å–ø–∏—Å–æ–∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è)")
                    
                # Store selected orders in session state
                st.session_state.selected_orders = all_selected_orders
                logger.info(f"–í—ã–±—Ä–∞–Ω–æ {len(all_selected_orders)} –∑–∞–∫–∞–∑–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                # Log only first few orders to avoid slowdown
                if len(all_selected_orders) <= 5:
                    for order in all_selected_orders:
                        logger.info(f"  –ó–∞–∫–∞–∑ {order.get('order_id', 'N/A')}: {order.get('article', 'N/A')}")
                else:
                    # Log only first 3 and last 2 for large lists
                    for order in all_selected_orders[:3]:
                        logger.info(f"  –ó–∞–∫–∞–∑ {order.get('order_id', 'N/A')}: {order.get('article', 'N/A')}")
                    logger.info(f"  ... (–ø—Ä–æ–ø—É—â–µ–Ω–æ {len(all_selected_orders) - 5} –∑–∞–∫–∞–∑–æ–≤) ...")
                    for order in all_selected_orders[-2:]:
                        logger.info(f"  –ó–∞–∫–∞–∑ {order.get('order_id', 'N/A')}: {order.get('article', 'N/A')}")
        else:
            st.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–µ–≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–µ—Å—è—Ü–∞—Ö")
            
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ Excel —Ñ–∞–π–ª–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ Excel: {e}")
        import traceback
        logger.error(f"–ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
        st.error("üí° **–í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:**")
        st.error("‚Ä¢ –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ Excel —Ñ–∞–π–ª –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (.xlsx)")
        st.error("‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –¥–∞—Ç –≤ —Ñ–∞–π–ª–µ") 
        st.error("‚Ä¢ –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ –¥–∞—Ç–∞—Ö –Ω–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, 30 —Ñ–µ–≤—Ä–∞–ª—è)")

# Initialize auto_loaded_files
auto_loaded_files = []

# DXF files will be loaded on demand during optimization
# This section shows what will be processed when optimization starts
if st.session_state.selected_orders:
    st.subheader("üìã –ì–æ—Ç–æ–≤—ã–µ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–∫–∞–∑—ã")
    
    st.success(f"‚úÖ –í—ã–±—Ä–∞–Ω–æ {len(st.session_state.selected_orders)} –∑–∞–∫–∞–∑–æ–≤")
    st.info("üí° DXF —Ñ–∞–π–ª—ã –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ –∫–Ω–æ–ø–∫–∏ '–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å–∫—Ä–æ–π' –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.")
    
    # Show preview of what will be loaded
    articles_found = []
    articles_not_found = []
    
    # Create a file-like object with name attribute
    class FileObj:
        def __init__(self, content, name):
            self.content = BytesIO(content)
            self.name = name
        def read(self):
            return self.content.read()
        def seek(self, pos):
            return self.content.seek(pos)

    @st.cache_data(ttl=300)  # Cache for 5 minutes
    def find_dxf_files_for_article(article, product_name=''):
        """Find DXF files for a given article by searching in the dxf_samples directory structure."""
        import re
        found_files = []
        
        logger.info(f"–ü–æ–∏—Å–∫ DXF —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞—Ä—Ç–∏–∫—É–ª–∞: '{article}', —Ç–æ–≤–∞—Ä: '{product_name}'")
        
        # Strategy 1: Search by product name (e.g., "AUDI A6 (C7) 4")
        if product_name and not found_files:
            # Extract brand and model from product name
            product_upper = product_name.upper()
            
            # Handle common brand name mappings
            brand_mapping = {
                'AUDI': 'AUDI',
                'BMW': 'BMW',
                'MERCEDES': 'MERCEDES',
                'VOLKSWAGEN': 'VOLKSWAGEN',
                'FORD': 'FORD',
                'TOYOTA': 'TOYOTA',
                'NISSAN': 'NISSAN',
                'HYUNDAI': 'HYUNDAI',
                'KIA': 'KIA',
                'CHERY': 'CHERY',
                'CHANGAN': 'CHANGAN'
            }
            
            # Find brand in product name
            detected_brand = None
            for brand_key, brand_folder in brand_mapping.items():
                if brand_key in product_upper:
                    detected_brand = brand_folder
                    break
            
            if detected_brand:
                brand_path = f"dxf_samples/{detected_brand}"
                logger.debug(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –±—Ä–µ–Ω–¥: {detected_brand}, –ø—É—Ç—å: {brand_path}")
                if os.path.exists(brand_path):
                    # Create search keywords from product name
                    product_keywords = []
                    
                    # Clean product name and extract model parts
                    model_part = product_upper.replace(detected_brand, '').strip()
                    
                    # Add full product name as keyword
                    product_keywords.append(product_name.lower())
                    
                    # Handle parentheses and extract parts
                    if '(' in model_part and ')' in model_part:
                        parentheses_content = re.findall(r'\((.*?)\)', model_part)
                        base_model = re.sub(r'\s*\([^)]*\)\s*', ' ', model_part).strip()
                        
                        product_keywords.extend([
                            base_model.lower(),
                            model_part.replace('(', '').replace(')', '').lower(),
                        ])
                        
                        for content in parentheses_content:
                            product_keywords.extend([
                                content.lower(),
                                f"{base_model} {content}".lower(),
                            ])
                    
                    # Extract individual parts
                    model_parts = re.sub(r'[^\w\s]', ' ', model_part).split()
                    product_keywords.extend([part.lower() for part in model_parts if len(part) > 1])
                    
                    # Remove duplicates
                    product_keywords = list(set([k.strip() for k in product_keywords if k.strip()]))
                    
                    # Search through brand folders
                    for model_folder in os.listdir(brand_path):
                        model_folder_path = os.path.join(brand_path, model_folder)
                        if os.path.isdir(model_folder_path):
                            folder_name_lower = model_folder.lower()
                            
                            # Calculate match score with Cyrillic/Latin normalization
                            match_score = 0
                            
                            # Normalize folder name for better matching (handle Cyrillic/Latin)
                            normalized_folder = folder_name_lower
                            # Replace common Cyrillic letters with Latin equivalents
                            cyrillic_to_latin = {
                                '–∞': 'a', '–ê': 'A',
                                '—Å': 'c', '–°': 'C',
                                '–µ': 'e', '–ï': 'E',
                                '–æ': 'o', '–û': 'O',
                                '—Ä': 'p', '–†': 'P',
                                '—Ö': 'x', '–•': 'X'
                            }
                            for cyrillic, latin in cyrillic_to_latin.items():
                                normalized_folder = normalized_folder.replace(cyrillic, latin)
                            
                            for keyword in product_keywords:
                                if keyword and len(keyword) > 2:
                                    # Direct match in normalized folder name
                                    if keyword in normalized_folder:
                                        match_score += len(keyword) * 2
                                    # Direct match in original folder name
                                    elif keyword in folder_name_lower:
                                        match_score += len(keyword) * 2
                                    else:
                                        # Check partial matches in normalized folder
                                        keyword_parts = keyword.split()
                                        matched_parts = sum(1 for part in keyword_parts 
                                                          if part in normalized_folder or part in folder_name_lower)
                                        if matched_parts > 0:
                                            match_score += matched_parts * 3
                            
                            # If we have a good match, look for DXF files
                            if match_score >= 6:  # Threshold for product name matching
                                dxf_folder = os.path.join(model_folder_path, "DXF")
                                if os.path.exists(dxf_folder):
                                    dxf_files_found = [f for f in os.listdir(dxf_folder) if f.lower().endswith('.dxf')]
                                    for dxf_file in dxf_files_found:
                                        found_files.append(os.path.join(dxf_folder, dxf_file))
                                    if found_files:
                                        logger.debug(f"–ù–∞–π–¥–µ–Ω—ã DXF —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ {dxf_folder}: {len(dxf_files_found)} —Ñ–∞–π–ª–æ–≤")
                                        break
                                else:
                                    # Check for DXF files directly in model folder
                                    dxf_files_found = [f for f in os.listdir(model_folder_path) if f.lower().endswith('.dxf')]
                                    for dxf_file in dxf_files_found:
                                        found_files.append(os.path.join(model_folder_path, dxf_file))
                                    if found_files:
                                        break
        
        # Strategy 2: Direct path mapping (if article matches folder structure)
        if not found_files:
            direct_path = f"dxf_samples/{article}"
            if os.path.exists(direct_path):
                dxf_files = [f for f in os.listdir(direct_path) if f.lower().endswith('.dxf')]
                for dxf_file in dxf_files:
                    found_files.append(os.path.join(direct_path, dxf_file))
        
        # Strategy 2: Parse article and search in brand folders
        if not found_files and '+' in article:
            parts = article.split('+')
            if len(parts) >= 3:
                # Extract brand and model (e.g., EVA_BORT+Chery+Tiggo 4 -> Chery, Tiggo 4)
                brand = parts[1].strip()
                model_info = parts[2].strip() if len(parts) > 2 else ""
                
                # Create search variants for the brand
                brand_variants = [
                    brand.upper(),
                    brand.capitalize(),
                    brand.lower()
                ]
                
                # Find matching brand folder
                for brand_variant in brand_variants:
                    brand_path = f"dxf_samples/{brand_variant}"
                    if os.path.exists(brand_path):
                        # Look for model folders that might match
                        for model_folder in os.listdir(brand_path):
                            model_folder_path = os.path.join(brand_path, model_folder)
                            if os.path.isdir(model_folder_path):
                                # Create search keywords from model info
                                search_keywords = []
                                
                                # Clean model info and create variants
                                if model_info:
                                    # Handle specific transformations
                                    model_variants = [model_info]
                                    
                                    # Handle parentheses (e.g., "A6 (C7) 4" -> "A6", "A6 C7", "A6 4", "A6 C7 4")
                                    if '(' in model_info and ')' in model_info:
                                        # Extract parts from parentheses
                                        parentheses_content = re.findall(r'\((.*?)\)', model_info)
                                        base_model = re.sub(r'\s*\([^)]*\)\s*', ' ', model_info).strip()
                                        
                                        # Create variants with and without parentheses content
                                        model_variants.extend([
                                            base_model,  # "A6 4"
                                            model_info.replace('(', '').replace(')', ''),  # "A6 C7 4"
                                        ])
                                        
                                        # Add variants with parentheses content integrated
                                        for content in parentheses_content:
                                            model_variants.extend([
                                                f"{base_model} {content}",  # "A6 4 C7"
                                                f"{content} {base_model}",  # "C7 A6 4"
                                                content,  # "C7"
                                            ])
                                    
                                    # Handle "CX35PLUS" -> "CS35", "CS35 PLUS" 
                                    if 'CX35PLUS' in model_info:
                                        model_variants.append(model_info.replace('CX35PLUS', 'CS35'))
                                        model_variants.append(model_info.replace('CX35PLUS', 'CS35 PLUS'))
                                    
                                    # Handle "PLUS" variations
                                    if 'PLUS' in model_info:
                                        model_variants.append(model_info.replace('PLUS', ' PLUS'))
                                        model_variants.append(model_info.replace('PLUS', ''))
                                    
                                    # Handle "PRO" variations
                                    if 'PRO' in model_info:
                                        model_variants.append(model_info.replace('PRO', ' PRO'))
                                        model_variants.append(model_info.replace('PRO', ''))
                                    
                                    # Extract individual model parts (e.g., "A6 (C7) 4" -> ["A6", "C7", "4"])
                                    model_parts = re.sub(r'[^\w\s]', ' ', model_info).split()
                                    model_variants.extend(model_parts)
                                    
                                    # Create combinations of model parts
                                    if len(model_parts) >= 2:
                                        for i in range(len(model_parts)):
                                            for j in range(i+1, len(model_parts)+1):
                                                combination = ' '.join(model_parts[i:j])
                                                if len(combination.strip()) > 1:
                                                    model_variants.append(combination)
                                    
                                    # Create search keywords from all variants
                                    for variant in model_variants:
                                        search_keywords.extend([
                                            variant.lower().strip(),
                                            variant.replace(' ', '').lower(),
                                            variant[:10].lower().strip()  # First 10 chars
                                        ])
                                    
                                    # Remove duplicates and empty strings
                                    search_keywords = list(set([k for k in search_keywords if k.strip()]))
                                
                                # Check if this model folder matches our search keywords
                                folder_name_lower = model_folder.lower()
                                match_found = False
                                match_score = 0
                                
                                # Calculate match score for better ranking
                                for keyword in search_keywords:
                                    if keyword and len(keyword) > 1:  # Check even short keywords
                                        keyword_parts = keyword.split()
                                        current_score = 0
                                        
                                        if len(keyword_parts) >= 2:
                                            # Multi-word keyword like "a6 c7" or "a6 4"
                                            matched_parts = sum(1 for part in keyword_parts if part in folder_name_lower)
                                            if matched_parts == len(keyword_parts):
                                                current_score = 10 + len(keyword_parts)  # High score for complete matches
                                                match_found = True
                                            elif matched_parts > 0:
                                                current_score = matched_parts * 2  # Partial match score
                                        else:
                                            # Single word keyword
                                            if keyword in folder_name_lower:
                                                # Exact substring match
                                                current_score = 8 + len(keyword)
                                                match_found = True
                                            elif len(keyword) >= 3:
                                                # Check for partial matches in folder name parts
                                                folder_parts = re.split(r'[\s\-_]', folder_name_lower)
                                                for folder_part in folder_parts:
                                                    if keyword in folder_part or folder_part in keyword:
                                                        current_score = max(current_score, 3)
                                                        match_found = True
                                        
                                        match_score = max(match_score, current_score)
                                
                                # Only proceed if we have a reasonable match
                                if match_found and match_score >= 3:
                                    # Look for DXF folder first
                                    dxf_folder = os.path.join(model_folder_path, "DXF")
                                    if os.path.exists(dxf_folder):
                                        dxf_files = [f for f in os.listdir(dxf_folder) if f.lower().endswith('.dxf')]
                                        for dxf_file in dxf_files:
                                            found_files.append(os.path.join(dxf_folder, dxf_file))
                                        if found_files:
                                            break
                                    else:
                                        # Look for DXF files directly in model folder
                                        dxf_files = [f for f in os.listdir(model_folder_path) if f.lower().endswith('.dxf')]
                                        for dxf_file in dxf_files:
                                            found_files.append(os.path.join(model_folder_path, dxf_file))
                                        if found_files:
                                            break
                        
                        if found_files:
                            break
        
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –¥–ª—è '{article}': –Ω–∞–π–¥–µ–Ω–æ {len(found_files)} —Ñ–∞–π–ª–æ–≤")
        if found_files:
            logger.debug(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {[os.path.basename(f) for f in found_files]}")
        return found_files

    # Show selected orders without file system search
    st.info(f"üìä –ì–æ—Ç–æ–≤–æ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: **{len(st.session_state.selected_orders)}** –∑–∞–∫–∞–∑–æ–≤")
    
    # Show selected orders in a compact format
    with st.expander("üìã –°–ø–∏—Å–æ–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤", expanded=False):
        for i, order in enumerate(st.session_state.selected_orders, 1):
            st.write(f"{i}. **{order['product']}** (–∑–∞–∫–∞–∑: {order.get('order_id', 'N/A')})")
    
    st.success("‚úÖ DXF —Ñ–∞–π–ª—ã –±—É–¥—É—Ç –Ω–∞–π–¥–µ–Ω—ã –∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")

# Additional DXF files section (shown when orders are selected)
if st.session_state.selected_orders:
    st.subheader("üìé –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ DXF —Ñ–∞–π–ª—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")
    manual_files = st.file_uploader("–î–æ–±–∞–≤—å—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ DXF —Ñ–∞–π–ª—ã –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏", type=["dxf"], accept_multiple_files=True, key="manual_dxf")
    
    # Store manual files in session state for later processing
    if manual_files:
        st.write("**–í—ã–±–µ—Ä–∏—Ç–µ —Ü–≤–µ—Ç –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:**")
        manual_color = st.selectbox(
            "–¶–≤–µ—Ç –ª–∏—Å—Ç–∞ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:",
            options=["—Å–µ—Ä—ã–π", "—á—ë—Ä–Ω—ã–π"],
            index=0,
            key="manual_files_color",
            help="–í—ã–±–µ—Ä–∏—Ç–µ —Ü–≤–µ—Ç –ª–∏—Å—Ç–∞, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞–∑–º–µ—â–µ–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã"
        )
        # Store manual files and color in session state
        st.session_state.manual_files = manual_files
        st.session_state.manual_color = manual_color
        st.success(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(manual_files)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
    else:
        st.session_state.manual_files = []
else:
    # No orders selected or no files found - show message  
    if st.session_state.selected_orders:
        st.warning("‚ö†Ô∏è –î–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã DXF —Ñ–∞–π–ª—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ dxf_samples.")
    else:
        st.info("üí° –í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–∫–∞–∑—ã –∏–∑ Excel —Ç–∞–±–ª–∏—Ü—ã –≤—ã—à–µ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ DXF —Ñ–∞–π–ª–æ–≤.")

if st.button("üöÄ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å–∫—Ä–æ–π"):
    logger.info("=== –ù–ê–ß–ê–õ–û –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –†–ê–°–ö–†–û–Ø ===")
    if not st.session_state.available_sheets:
        logger.error("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ª–∏—Å—Ç–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
        st.error("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–±–∞–≤—å—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ç–∏–ø –ª–∏—Å—Ç–∞ –≤ –Ω–∞–ª–∏—á–∏–∏.")
    elif not st.session_state.selected_orders:
        logger.error("–ù–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
        st.error("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –∑–∞–∫–∞–∑—ã –∏–∑ Excel —Ç–∞–±–ª–∏—Ü—ã.")
    else:
        # Now load DXF files on demand  
        st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ DXF —Ñ–∞–π–ª–æ–≤")
        
        # Load files from selected orders
        auto_loaded_files = []
        manual_files_with_color = []
        
        # Create FileObj class for this context
        class FileObj:
            def __init__(self, content, name):
                self.content = BytesIO(content)
                self.name = name
            def read(self):
                return self.content.read()
            def seek(self, pos):
                return self.content.seek(pos)
        
        # Load files from orders
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        total_orders = len(st.session_state.selected_orders)
        for i, order in enumerate(st.session_state.selected_orders):
            progress = (i + 1) / total_orders
            progress_bar.progress(progress)
            status_text.text(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –∑–∞–∫–∞–∑–∞ {i + 1}/{total_orders}: {order['product'][:50]}...")
            
            article = order['article']
            product = order['product']
            found_dxf_files = find_dxf_files_for_article(article, product)
            
            if found_dxf_files:
                for file_path in found_dxf_files:
                    try:
                        with open(file_path, 'rb') as f:
                            file_content = f.read()
                        
                        display_name = f"{product}_{os.path.basename(file_path)}"
                        file_obj = FileObj(file_content, display_name)
                        file_obj.color = order.get('color', '—Å–µ—Ä—ã–π')
                        file_obj.order_id = order.get('order_id', 'unknown')
                        auto_loaded_files.append(file_obj)
                        logger.debug(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª {display_name} –¥–ª—è –∑–∞–∫–∞–∑–∞ {file_obj.order_id}")
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
            else:
                st.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã DXF —Ñ–∞–π–ª—ã –¥–ª—è –∑–∞–∫–∞–∑–∞: {product}")
        
        # Load manual files if any
        if hasattr(st.session_state, 'manual_files') and st.session_state.manual_files:
            for file in st.session_state.manual_files:
                file.color = getattr(st.session_state, 'manual_color', '—Å–µ—Ä—ã–π')
                file.order_id = 'additional'
                manual_files_with_color.append(file)
        
        # Combine all files
        dxf_files = auto_loaded_files + manual_files_with_color
        
        progress_bar.empty()
        status_text.text(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dxf_files)} —Ñ–∞–π–ª–æ–≤ ({len(auto_loaded_files)} –∏–∑ –∑–∞–∫–∞–∑–æ–≤, {len(manual_files_with_color)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö)")
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å {len(dxf_files)} DXF —Ñ–∞–π–ª–∞–º–∏ –∏ {len(st.session_state.available_sheets)} —Ç–∏–ø–∞–º–∏ –ª–∏—Å—Ç–æ–≤")
        # Parse DXF files
        st.header("üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ DXF —Ñ–∞–π–ª–æ–≤")
        polygons = []
        original_dxf_data_map = {}  # Store original DXF data for each file
        
        # Parse loaded DXF files
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ DXF —Ñ–∞–π–ª–æ–≤...")
        
        # Show progress for file parsing
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, file in enumerate(dxf_files):
            # Update progress
            progress = (idx + 1) / len(dxf_files)
            progress_bar.progress(progress)
            status_text.text(f"–ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª {idx + 1}/{len(dxf_files)}: {file.name}")
            
            file.seek(0)
            file_bytes = BytesIO(file.read())
            parsed_data = parse_dxf_complete(file_bytes, verbose=False)
            if parsed_data and parsed_data['combined_polygon']:
                # Add color and order information to polygon tuple
                file_color = getattr(file, 'color', '—Å–µ—Ä—ã–π')
                file_order_id = getattr(file, 'order_id', 'unknown')
                
                # DEBUG: Log all file attributes to understand the issue
                file_attrs = [attr for attr in dir(file) if not attr.startswith('_')]
                logger.debug(f"–§–ê–ô–õ {file.name}: –∞—Ç—Ä–∏–±—É—Ç—ã = {file_attrs}")
                logger.debug(f"–§–ê–ô–õ {file.name}: color = {file_color}, order_id = {file_order_id}")
                
                # Use the combined polygon with extended format: (polygon, filename, color, order_id)
                polygon_tuple = (parsed_data['combined_polygon'], file.name, file_color, file_order_id)
                polygons.append(polygon_tuple)
                logger.info(f"–î–û–ë–ê–í–õ–ï–ù –ü–û–õ–ò–ì–û–ù: tuple –¥–ª–∏–Ω–∞={len(polygon_tuple)}, order_id={polygon_tuple[3] if len(polygon_tuple) > 3 else '–ù–ï–¢'}")
                # Store original DXF data for this file
                original_dxf_data_map[file.name] = parsed_data
                logger.info(f"–°–û–ó–î–ê–ù –ü–û–õ–ò–ì–û–ù: —Ñ–∞–π–ª={file.name}, –∑–∞–∫–∞–∑={file_order_id}, —Ü–≤–µ—Ç={file_color}")
            else:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–∏–≥–æ–Ω –∏–∑ —Ñ–∞–π–ª–∞ {file.name}")
        
        # Clear progress indicators
        progress_bar.empty()
        status_text.text(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(polygons)} –ø–æ–ª–∏–≥–æ–Ω–æ–≤ –∏–∑ {len(dxf_files)} —Ñ–∞–π–ª–æ–≤")
        
        # Show detailed parsing info in expander
        with st.expander("üîç –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ñ–∞–π–ª–æ–≤", expanded=False):
            st.write("–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É–ª—É—á—à–µ–Ω–Ω–æ–º –ø–∞—Ä—Å–∏–Ω–≥–µ —Ñ–∞–π–ª–æ–≤:")
            for file in dxf_files:
                st.write(f"**–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {file.name}**")
                file.seek(0)
                file_bytes = BytesIO(file.read())
                parse_dxf_complete(file_bytes, verbose=True)
        
        if not polygons:
            st.error("–í –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö DXF —Ñ–∞–π–ª–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–æ–ª–∏–≥–æ–Ω–æ–≤!")
            st.stop()
        
        # Show order distribution before optimization
        order_counts = {}
        for polygon_tuple in polygons:
            if len(polygon_tuple) >= 4:
                order_id = polygon_tuple[3]
                order_counts[order_id] = order_counts.get(order_id, 0) + 1
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–∫–∞–∑–æ–≤: –Ω–∞–π–¥–µ–Ω–æ {len(order_counts)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤")
        for order_id, count in order_counts.items():
            logger.info(f"  ‚Ä¢ –ó–∞–∫–∞–∑ {order_id}: {count} —Ñ–∞–π–ª–æ–≤")
        
        st.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(order_counts)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤:")
        for order_id, count in order_counts.items():
            st.write(f"  ‚Ä¢ –ó–∞–∫–∞–∑ {order_id}: {count} —Ñ–∞–π–ª–æ–≤")
        
        # Show input visualization
        st.header("üîç –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        input_plots = plot_input_polygons(polygons)
        if input_plots:
            # Show color legend
            with st.expander("üé® –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞ —Ñ–∞–π–ª–æ–≤", expanded=False):
                legend_cols = st.columns(min(5, len(polygons)))
                for i, polygon_tuple in enumerate(polygons):
                    if len(polygon_tuple) >= 3:  # New format with color
                        _, file_name, _ = polygon_tuple[:3]
                    else:  # Old format without color
                        _, file_name = polygon_tuple[:2]
                    with legend_cols[i % len(legend_cols)]:
                        from layout_optimizer import get_color_for_file
                        color = get_color_for_file(file_name)
                        # Convert RGB to hex for HTML
                        color_hex = f"#{int(color[0]*255):02x}{int(color[1]*255):02x}{int(color[2]*255):02x}"
                        st.markdown(f'<div style="background-color: {color_hex}; padding: 10px; border-radius: 5px; text-align: center; margin: 2px;"><b>{file_name}</b></div>', 
                                  unsafe_allow_html=True)
            
            cols = st.columns(min(3, len(input_plots)))
            for i, (file_name, plot_buf) in enumerate(input_plots.items()):
                with cols[i % len(cols)]:
                    st.image(plot_buf, caption=f"{file_name}", use_container_width=True)
        
        # Show polygon statistics with real dimensions
        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        
        # Store original dimensions for comparison later
        original_dimensions = {}
        
        # Create a summary table with proper unit conversion
        summary_data = []
        total_area_cm2 = 0
        for polygon_tuple in polygons:
            if len(polygon_tuple) >= 4:  # Extended format with color and order_id
                poly, filename, color, order_id = polygon_tuple[:4]
            elif len(polygon_tuple) >= 3:  # Format with color
                poly, filename, color = polygon_tuple[:3]
                order_id = 'unknown'
            else:  # Old format without color
                poly, filename = polygon_tuple[:2]
                color = '—Å–µ—Ä—ã–π'
                order_id = 'unknown'
            bounds = poly.bounds
            width_mm = bounds[2] - bounds[0]
            height_mm = bounds[3] - bounds[1]
            area_mm2 = poly.area
            
            # Convert from mm to cm
            width_cm = width_mm / 10.0
            height_cm = height_mm / 10.0
            area_cm2 = area_mm2 / 100.0
            
            # Store original dimensions
            original_dimensions[filename] = {
                "width_cm": width_cm,
                "height_cm": height_cm,
                "area_cm2": area_cm2
            }
            
            total_area_cm2 += area_cm2
            # Add color emoji for display
            color_emoji = "‚ö´" if color == "—á—ë—Ä–Ω—ã–π" else "‚ö™" if color == "—Å–µ—Ä—ã–π" else "üîò"
            color_display = f"{color_emoji} {color}"
            
            summary_data.append({
                "–§–∞–π–ª": filename,
                "–®–∏—Ä–∏–Ω–∞ (—Å–º)": f"{width_cm:.1f}",
                "–í—ã—Å–æ—Ç–∞ (—Å–º)": f"{height_cm:.1f}",
                "–ü–ª–æ—â–∞–¥—å (—Å–º¬≤)": f"{area_cm2:.2f}",
                "–¶–≤–µ—Ç": color_display,
                "–ó–∞–∫–∞–∑": order_id if order_id != 'unknown' else '-'
            })
        
        summary_df = pd.DataFrame(summary_data)
        st.dataframe(summary_df, use_container_width=True)
        
        # Calculate theoretical minimum using largest available sheet
        largest_sheet_area = max(sheet['width'] * sheet['height'] for sheet in st.session_state.available_sheets)
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("–í—Å–µ–≥–æ –æ–±—ä–µ–∫—Ç–æ–≤", len(polygons))
        with col2:
            st.metric("–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å", f"{total_area_cm2:.2f} —Å–º¬≤")
        with col3:
            st.metric("–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –º–∏–Ω–∏–º—É–º –ª–∏—Å—Ç–æ–≤", max(1, int(np.ceil(total_area_cm2 / largest_sheet_area))))
        
        # Auto-scale polygons if needed (use largest available sheet for scaling reference)
        st.header("üìê –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–∏–≥–æ–Ω–æ–≤")
        
        # Find largest sheet for scaling reference
        max_sheet_area = 0
        reference_sheet_size = (140, 200)  # default fallback
        for sheet in st.session_state.available_sheets:
            area = sheet['width'] * sheet['height']
            if area > max_sheet_area:
                max_sheet_area = area
                reference_sheet_size = (sheet['width'], sheet['height'])
        
        # Scale quietly first
        scaled_polygons = scale_polygons_to_fit(polygons, reference_sheet_size, verbose=False)
        
        # Show scaling details in expander
        with st.expander("üîç –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏", expanded=False):
            st.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –ª–∏—Å—Ç ({reference_sheet_size[0]}x{reference_sheet_size[1]} —Å–º) –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")
            scale_polygons_to_fit(polygons, reference_sheet_size, verbose=True)
        
        polygons = scaled_polygons
        
        st.header("üîÑ –ü—Ä–æ—Ü–µ—Å—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")

        try:
            # Debug processing with detailed info
            with st.expander("üîç –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏", expanded=False):
                debug_layouts, debug_unplaced = bin_packing_with_inventory(polygons, st.session_state.available_sheets, verbose=True, max_sheets_per_order=MAX_SHEETS_PER_ORDER)
            
            # Actual processing (quiet)
            logger.info(f"–í—ã–∑—ã–≤–∞–µ–º bin_packing_with_inventory —Å MAX_SHEETS_PER_ORDER={MAX_SHEETS_PER_ORDER}")
            logger.info(f"–í—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {len(polygons)} –ø–æ–ª–∏–≥–æ–Ω–æ–≤, {len(st.session_state.available_sheets)} —Ç–∏–ø–æ–≤ –ª–∏—Å—Ç–æ–≤")
            
            # DEBUG: Log what polygons we're sending
            logger.info("–ü–û–õ–ò–ì–û–ù–´ –ü–ï–†–ï–î –û–¢–ü–†–ê–í–ö–û–ô –í bin_packing_with_inventory:")
            for i, polygon_tuple in enumerate(polygons):
                if len(polygon_tuple) >= 4:
                    logger.info(f"  –ü–æ–ª–∏–≥–æ–Ω {i}: —Ñ–∞–π–ª={polygon_tuple[1]}, order_id={polygon_tuple[3]}")
                else:
                    logger.warning(f"  –ü–æ–ª–∏–≥–æ–Ω {i}: –Ω–µ–ø–æ–ª–Ω—ã–π tuple (–¥–ª–∏–Ω–∞={len(polygon_tuple)})")
            
            placed_layouts, unplaced_polygons = bin_packing_with_inventory(polygons, st.session_state.available_sheets, verbose=False, max_sheets_per_order=MAX_SHEETS_PER_ORDER)
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç bin_packing: {len(placed_layouts)} —Ä–∞–∑–º–µ—â–µ–Ω–Ω—ã—Ö –ª–∏—Å—Ç–æ–≤, {len(unplaced_polygons)} –Ω–µ—Ä–∞–∑–º–µ—â–µ–Ω–Ω—ã—Ö –ø–æ–ª–∏–≥–æ–Ω–æ–≤")
        
        except ValueError as e:
            # Handle order constraint violations
            if "–ù–∞—Ä—É—à–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∑–∞–∫–∞–∑–æ–≤" in str(e):
                st.error(f"‚ùå {str(e)}")
                st.info(f"üí° **–†–µ—à–µ–Ω–∏–µ**: –£–≤–µ–ª–∏—á—å—Ç–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É MAX_SHEETS_PER_ORDER (—Å–µ–π—á–∞—Å: {MAX_SHEETS_PER_ORDER}) –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ —Ñ–∞–π–ª—ã –∑–∞–∫–∞–∑–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–µ–π.")
                st.stop()
            else:
                # Re-raise other ValueError exceptions
                raise
        
        # Convert to old format for compatibility with existing display code
        all_layouts = []
        report_data = []
        
        for i, layout in enumerate(placed_layouts):
            # Save and visualize layout with new naming format: length_width_number.dxf
            sheet_width = int(layout['sheet_size'][0])
            sheet_height = int(layout['sheet_size'][1]) 
            sheet_number = layout['sheet_number']
            output_filename = f"{sheet_height}_{sheet_width}_{sheet_number}.dxf"
            output_file = os.path.join(OUTPUT_FOLDER, output_filename)
            save_dxf_layout_complete(layout['placed_polygons'], layout['sheet_size'], output_file, original_dxf_data_map)
            layout_plot = plot_layout(layout['placed_polygons'], layout['sheet_size'])

            # Find sheet color from original sheet data
            sheet_color = "–Ω–µ —É–∫–∞–∑–∞–Ω"
            for sheet in st.session_state.available_sheets:
                if sheet['name'] == layout['sheet_type']:
                    sheet_color = sheet.get('color', '–Ω–µ —É–∫–∞–∑–∞–Ω')
                    break
            
            # Store layout info in old format for compatibility
            all_layouts.append({
                "Sheet": layout['sheet_number'],
                "Sheet Type": layout['sheet_type'],
                "Sheet Color": sheet_color,
                "Sheet Size": f"{layout['sheet_size'][0]}x{layout['sheet_size'][1]} —Å–º",
                "Output File": output_file,
                "Plot": layout_plot,
                "Shapes Placed": len(layout['placed_polygons']),
                "Material Usage (%)": f"{layout['usage_percent']:.2f}",
                "Placed Polygons": layout['placed_polygons']
            })
            report_data.extend([(p[4], layout['sheet_number'], output_file) for p in layout['placed_polygons']])
        
        # Update sheet inventory in session state
        for layout in placed_layouts:
            for original_sheet in st.session_state.available_sheets:
                if layout['sheet_type'] == original_sheet['name']:
                    original_sheet['used'] += 1
                    break

        # Display Results
        st.header("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        if all_layouts:
            st.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ª–∏—Å—Ç–æ–≤: {len(all_layouts)}")
            
            # Summary statistics
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("–í—Å–µ–≥–æ –ª–∏—Å—Ç–æ–≤", len(all_layouts))
            with col2:
                total_placed = sum(layout["Shapes Placed"] for layout in all_layouts)
                st.metric("–†–∞–∑–º–µ—â–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤", f"{total_placed}/{len(polygons)}")
            with col3:
                avg_usage = sum(float(layout["Material Usage (%)"].replace('%', '')) for layout in all_layouts) / len(all_layouts)
                st.metric("–°—Ä–µ–¥–Ω–∏–π —Ä–∞—Å—Ö–æ–¥ –º–∞—Ç–µ—Ä–∏–∞–ª–∞", f"{avg_usage:.1f}%")
            with col4:
                if unplaced_polygons:
                    st.metric("–ù–µ —Ä–∞–∑–º–µ—â–µ–Ω–æ", len(unplaced_polygons), delta=f"-{len(unplaced_polygons)}", delta_color="inverse")
                else:
                    st.metric("–ù–µ —Ä–∞–∑–º–µ—â–µ–Ω–æ", 0, delta="–í—Å–µ —Ä–∞–∑–º–µ—â–µ–Ω–æ ‚úÖ")
            
            # Show updated inventory
            st.subheader("üì¶ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å –ª–∏—Å—Ç–æ–≤")
            updated_sheets_data = []
            for sheet in st.session_state.available_sheets:
                # Add color indicator
                color = sheet.get('color', '–Ω–µ —É–∫–∞–∑–∞–Ω')
                color_emoji = "‚ö´" if color == "—á—ë—Ä–Ω—ã–π" else "‚ö™" if color == "—Å–µ—Ä—ã–π" else "üîò"
                color_display = f"{color_emoji} {color}"
                
                updated_sheets_data.append({
                    "–¢–∏–ø –ª–∏—Å—Ç–∞": sheet['name'],
                    "–†–∞–∑–º–µ—Ä (—Å–º)": f"{sheet['width']}x{sheet['height']}",
                    "–¶–≤–µ—Ç": color_display,
                    "–ë—ã–ª–æ": sheet['count'],
                    "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ": sheet['used'],
                    "–û—Å—Ç–∞–ª–æ—Å—å": sheet['count'] - sheet['used']
                })
            updated_df = pd.DataFrame(updated_sheets_data)
            st.dataframe(updated_df, use_container_width=True)
            
            # Detailed results table with sizes
            st.subheader("üìã –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            
            # Create enhanced report with sizes
            enhanced_report_data = []
            for layout in all_layouts:
                for placed_tuple in layout["Placed Polygons"]:
                    if len(placed_tuple) >= 6:  # New format with color
                        polygon, _, _, angle, file_name, color = placed_tuple[:6]
                    else:  # Old format without color
                        polygon, _, _, angle, file_name = placed_tuple[:5]
                        color = '—Å–µ—Ä—ã–π'
                    bounds = polygon.bounds
                    width_cm = (bounds[2] - bounds[0]) / 10
                    height_cm = (bounds[3] - bounds[1]) / 10
                    area_cm2 = polygon.area / 100
                    
                    # Compare with original dimensions
                    original = original_dimensions.get(file_name, {})
                    original_width = original.get("width_cm", 0)
                    original_height = original.get("height_cm", 0)
                    original_area = original.get("area_cm2", 0)
                    
                    scale_factor = (width_cm / original_width) if original_width > 0 else 1.0
                    
                    size_comparison = f"{width_cm:.1f}√ó{height_cm:.1f}"
                    if abs(scale_factor - 1.0) > 0.01:  # If scaled
                        size_comparison += f" (–±—ã–ª–æ {original_width:.1f}√ó{original_height:.1f})"
                    
                    enhanced_report_data.append({
                        "DXF —Ñ–∞–π–ª": file_name,
                        "–ù–æ–º–µ—Ä –ª–∏—Å—Ç–∞": layout["Sheet"],
                        "–†–∞–∑–º–µ—Ä (—Å–º)": size_comparison,
                        "–ü–ª–æ—â–∞–¥—å (—Å–º¬≤)": f"{area_cm2:.2f}",
                        "–ü–æ–≤–æ—Ä–æ—Ç (¬∞)": f"{angle:.0f}",
                        "–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª": layout["Output File"]
                    })
            
            if enhanced_report_data:
                enhanced_df = pd.DataFrame(enhanced_report_data)
                st.dataframe(enhanced_df, use_container_width=True)
                # Also create simple report_df for export
                report_df = pd.DataFrame(report_data, columns=["DXF —Ñ–∞–π–ª", "–ù–æ–º–µ—Ä –ª–∏—Å—Ç–∞", "–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª"])
            else:
                report_df = pd.DataFrame(report_data, columns=["DXF —Ñ–∞–π–ª", "–ù–æ–º–µ—Ä –ª–∏—Å—Ç–∞", "–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª"])
                st.dataframe(report_df, use_container_width=True)

            # Sheet visualizations
            st.subheader("üìê –°—Ö–µ–º—ã —Ä–∞—Å–∫—Ä–æ—è –ª–∏—Å—Ç–æ–≤")
            for layout in all_layouts:
                # Add color indicator emoji
                color_emoji = "‚ö´" if layout['Sheet Color'] == "—á—ë—Ä–Ω—ã–π" else "‚ö™" if layout['Sheet Color'] == "—Å–µ—Ä—ã–π" else "üîò"
                
                st.write(f"**–õ–∏—Å—Ç ‚Ññ{layout['Sheet']}: {color_emoji} {layout['Sheet Type']} ({layout['Sheet Size']}) - {layout['Shapes Placed']} –æ–±—ä–µ–∫—Ç–æ–≤ - {layout['Material Usage (%)']}% —Ä–∞—Å—Ö–æ–¥**")
                col1, col2 = st.columns([2, 1])
                with col1:
                    st.image(layout["Plot"], caption=f"–†–∞—Å–∫—Ä–æ–π –ª–∏—Å—Ç–∞ ‚Ññ{layout['Sheet']} ({layout['Sheet Type']})", use_container_width=True)
                with col2:
                    st.write(f"**–¢–∏–ø –ª–∏—Å—Ç–∞:** {layout['Sheet Type']}")
                    st.write(f"**–¶–≤–µ—Ç –ª–∏—Å—Ç–∞:** {color_emoji} {layout['Sheet Color']}")
                    st.write(f"**–†–∞–∑–º–µ—Ä –ª–∏—Å—Ç–∞:** {layout['Sheet Size']}")
                    st.write(f"**–†–∞–∑–º–µ—â–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤:** {layout['Shapes Placed']}")
                    st.write(f"**–†–∞—Å—Ö–æ–¥ –º–∞—Ç–µ—Ä–∏–∞–ª–∞:** {layout['Material Usage (%)']}%")
                    with open(layout["Output File"], "rb") as f:
                        st.download_button(
                            label=f"üì• –°–∫–∞—á–∞—Ç—å DXF",
                            data=f,
                            file_name=os.path.basename(layout["Output File"]),
                            mime="application/dxf",
                            key=f"download_{layout['Sheet']}"
                        )
                st.divider()  # Add visual separator between sheets
        else:
            st.error("‚ùå –ù–µ –±—ã–ª–æ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –ª–∏—Å—Ç–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤—ã—à–µ.")
        
        # Show unplaced polygons if any
        if unplaced_polygons:
            st.warning(f"‚ö†Ô∏è {len(unplaced_polygons)} –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–º–µ—Å—Ç–∏—Ç—å.")
            st.subheader("üö´ –ù–µ—Ä–∞–∑–º–µ—â–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã")
            unplaced_data = []
            for polygon_tuple in unplaced_polygons:
                if len(polygon_tuple) >= 3:  # New format with color
                    poly, name, color = polygon_tuple[:3]
                else:  # Old format without color
                    poly, name = polygon_tuple[:2]
                    color = '—Å–µ—Ä—ã–π'
                unplaced_data.append((name, f"{poly.area/100:.2f}", color))
            
            unplaced_df = pd.DataFrame(unplaced_data, columns=["–§–∞–π–ª", "–ü–ª–æ—â–∞–¥—å (—Å–º¬≤)", "–¶–≤–µ—Ç"])
            st.dataframe(unplaced_df, use_container_width=True)

        # Save report
        if all_layouts:
            report_file = os.path.join(OUTPUT_FOLDER, f"layout_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")
            
            # Use enhanced report if available, otherwise simple report
            if enhanced_report_data:
                enhanced_df.to_excel(report_file, index=False)
            elif 'report_df' in locals():
                report_df.to_excel(report_file, index=False)
            else:
                # Fallback: create simple report from report_data
                fallback_df = pd.DataFrame(report_data, columns=["DXF —Ñ–∞–π–ª", "–ù–æ–º–µ—Ä –ª–∏—Å—Ç–∞", "–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª"])
                fallback_df.to_excel(report_file, index=False)
            
            # Create ZIP archive with all DXF files
            zip_filename = f"layout_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
            zip_path = os.path.join(OUTPUT_FOLDER, zip_filename)
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Add all DXF layout files
                for layout in all_layouts:
                    dxf_file_path = layout["Output File"]
                    if os.path.exists(dxf_file_path):
                        # Use the new naming format for files in zip
                        arcname = os.path.basename(dxf_file_path)
                        zipf.write(dxf_file_path, arcname)
                
                # Add report file
                if os.path.exists(report_file):
                    zipf.write(report_file, os.path.basename(report_file))
            
            # Download buttons
            col1, col2 = st.columns([1, 1])
            
            with col1:
                with open(report_file, "rb") as f:
                    st.download_button(
                        label="üìä –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç Excel",
                        data=f,
                        file_name=os.path.basename(report_file),
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
            
            with col2:
                with open(zip_path, "rb") as f:
                    st.download_button(
                        label="üì¶ –°–∫–∞—á–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã (ZIP)",
                        data=f,
                        file_name=zip_filename,
                        mime="application/zip"
                    )

# Footer
#st.write("–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º —É–ø–∞–∫–æ–≤–∫–∏. –î–ª—è –ª—É—á—à–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ BL-NFP.")